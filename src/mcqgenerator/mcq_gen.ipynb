{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI, OpenAIEmbeddings,ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pinecone import Pinecone\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acessing Pinecone database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 382}},\n",
      " 'total_vector_count': 382}\n"
     ]
    }
   ],
   "source": [
    "## Pinecone Initialization and acessing \n",
    "pc = Pinecone(api_key=\"pcsk_6hKpf2_MvwLm4V9pu54N9nmDmUY8MFNuaHrP43JBwLD2FEEMPjC1GhKNVc5AUmhHzobyMP\")\n",
    "index_name=\"medicalqabot\"\n",
    "\n",
    "print(pc.Index(index_name).describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_services(index_name,openai_api_key):\n",
    "\n",
    "    ## Pinecone Initialization and acessing \n",
    "    pc = Pinecone(api_key=\"pcsk_6hKpf2_MvwLm4V9pu54N9nmDmUY8MFNuaHrP43JBwLD2FEEMPjC1GhKNVc5AUmhHzobyMP\")\n",
    "    index_name=index_name\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "    ## llm initialization\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0.7,\n",
    "        model_name=\"gpt-4\",\n",
    "        openai_api_key=openai_api_key\n",
    "    )\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "\n",
    "    return index, llm, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mcq_prompt():\n",
    "    \"\"\"Return the chat prompt template for MCQ generation\"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "    Based on the following medical context, create a challenging and unique multiple choice question.\n",
    "    The question should test important clinical concepts and decision-making.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Consider these aspects when generating the question:\n",
    "    1. Focus on specific clinical scenarios rather than general knowledge\n",
    "    2. Include realistic laboratory values, imaging findings, or patient symptoms\n",
    "    3. Make the scenario complex enough to test clinical decision-making\n",
    "    4. Include time-sensitive elements or risk factors that affect the decision\n",
    "    5. Consider including relevant comorbidities or complications\n",
    "    \n",
    "    Generate a multiple choice question in valid JSON format with the following structure:\n",
    "    {{\n",
    "        \"question\": \"Create a detailed clinical scenario that includes:\n",
    "                    - Specific patient demographics\n",
    "                    - Precise symptoms and timeline\n",
    "                    - Relevant lab values or imaging results\n",
    "                    - Important comorbidities\n",
    "                    - Any critical time factors\",\n",
    "        \"options\": [\n",
    "            \"A) A specific, detailed intervention or treatment\",\n",
    "            \"B) An alternative approach with different timing or method\",\n",
    "            \"C) A reasonable but suboptimal choice given the specifics\",\n",
    "            \"D) A common misconception or clearly incorrect approach\"\n",
    "        ],\n",
    "        \"correct_answer\": \"Letter of correct answer (A, B, C, or D)\",\n",
    "        \"reasoning\": \"Provide detailed explanation including:\n",
    "                     1. Why the correct answer is optimal for this specific case\n",
    "                     2. Why each incorrect option is inappropriate\n",
    "                     3. Key clinical factors that influenced the decision\n",
    "                     4. Any relevant guidelines or evidence supporting the choice\"\n",
    "    }}\n",
    "    \n",
    "    Requirements for uniqueness:\n",
    "    1. Avoid basic or commonly tested scenarios\n",
    "    2. Include unique combinations of symptoms or findings\n",
    "    3. Make sure each option is distinct and specific\n",
    "    4. Focus on nuanced clinical decision-making\n",
    "    5. Include recent medical guidelines when relevant\n",
    "    \n",
    "    Return only the JSON object with no additional text or formatting.\n",
    "    \"\"\"\n",
    "    return ChatPromptTemplate.from_template(prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_contexts(index, num_contexts=100):\n",
    "    \"\"\"Retrieve diverse contexts from Pinecone\"\"\"\n",
    "    vectors = index.query(\n",
    "        vector=[0.0] * 1536,  # dummy vector to get all records\n",
    "        top_k=num_contexts,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    return [match.metadata.get('text', '') for match in vectors.matches]\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def calculate_relevance(embeddings, context, question):\n",
    "    \"\"\"Calculate semantic similarity between context and question\"\"\"\n",
    "    context_embedding = embeddings.embed_query(context)\n",
    "    question_embedding = embeddings.embed_query(question)\n",
    "    return float(cosine_similarity(context_embedding, question_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(index, chat_llm, embeddings, num_questions=100):\n",
    "    \"\"\"Generate MCQs from medical contexts\"\"\"\n",
    "    contexts = retrieve_contexts(index, num_questions)\n",
    "    questions = []\n",
    "    prompt = get_mcq_prompt()\n",
    "    \n",
    "    for i, context in enumerate(contexts):\n",
    "        try:\n",
    "            # Generate MCQ using ChatLLM\n",
    "            response = chat_llm.invoke(prompt.format_messages(context=context))\n",
    "            # Extract the content from AIMessage\n",
    "            mcq_str = response.content\n",
    "            # Parse the JSON string\n",
    "            mcq = json.loads(mcq_str)\n",
    "            \n",
    "            # Add metadata for retrieval testing\n",
    "            mcq['context_id'] = i\n",
    "            mcq['relevance_score'] = calculate_relevance(embeddings, context, mcq['question'])\n",
    "            \n",
    "            questions.append(mcq)\n",
    "            print(f\"Successfully generated question {i+1}/{num_questions}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating question {i+1}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_questions(questions, filename=\"medical_mcqs.json\"):\n",
    "    \"\"\"Save generated questions to a JSON file\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(questions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the MCQ generation and testing\"\"\"\n",
    "\n",
    "    # Set up credentials\n",
    "    openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    index_name = \"medicalqabot\" \n",
    "    \n",
    "    # Initialize services\n",
    "    index, llm, embeddings = initialize_services(\n",
    "        index_name,\n",
    "        openai_api_key\n",
    "    )\n",
    "    \n",
    "    # Generate questions\n",
    "    questions = generate_questions(index, llm, embeddings, num_questions=100)\n",
    "    \n",
    "    # Save questions\n",
    "    save_questions(questions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated question 1/100\n",
      "Successfully generated question 2/100\n",
      "Successfully generated question 3/100\n",
      "Successfully generated question 4/100\n",
      "Successfully generated question 5/100\n",
      "Successfully generated question 6/100\n",
      "Successfully generated question 7/100\n",
      "Successfully generated question 8/100\n",
      "Successfully generated question 9/100\n",
      "Successfully generated question 10/100\n",
      "Successfully generated question 11/100\n",
      "Successfully generated question 12/100\n",
      "Successfully generated question 13/100\n",
      "Successfully generated question 14/100\n",
      "Successfully generated question 15/100\n",
      "Successfully generated question 16/100\n",
      "Successfully generated question 17/100\n",
      "Successfully generated question 18/100\n",
      "Successfully generated question 19/100\n",
      "Successfully generated question 20/100\n",
      "Error generating question 21: Invalid control character at: line 10 column 537 (char 1298)\n",
      "Successfully generated question 22/100\n",
      "Successfully generated question 23/100\n",
      "Successfully generated question 24/100\n",
      "Successfully generated question 25/100\n",
      "Successfully generated question 26/100\n",
      "Successfully generated question 27/100\n",
      "Successfully generated question 28/100\n",
      "Successfully generated question 29/100\n",
      "Successfully generated question 30/100\n",
      "Successfully generated question 31/100\n",
      "Successfully generated question 32/100\n",
      "Successfully generated question 33/100\n",
      "Successfully generated question 34/100\n",
      "Successfully generated question 35/100\n",
      "Successfully generated question 36/100\n",
      "Successfully generated question 37/100\n",
      "Successfully generated question 38/100\n",
      "Successfully generated question 39/100\n",
      "Successfully generated question 40/100\n",
      "Successfully generated question 41/100\n",
      "Successfully generated question 42/100\n",
      "Error generating question 43: Invalid control character at: line 10 column 441 (char 1291)\n",
      "Successfully generated question 44/100\n",
      "Successfully generated question 45/100\n",
      "Error generating question 46: Invalid control character at: line 10 column 508 (char 1284)\n",
      "Successfully generated question 47/100\n",
      "Successfully generated question 48/100\n",
      "Successfully generated question 49/100\n",
      "Successfully generated question 50/100\n",
      "Successfully generated question 51/100\n",
      "Successfully generated question 52/100\n",
      "Successfully generated question 53/100\n",
      "Successfully generated question 54/100\n",
      "Successfully generated question 55/100\n",
      "Error generating question 56: Invalid control character at: line 10 column 303 (char 1176)\n",
      "Successfully generated question 57/100\n",
      "Successfully generated question 58/100\n",
      "Successfully generated question 59/100\n",
      "Successfully generated question 60/100\n",
      "Successfully generated question 61/100\n",
      "Successfully generated question 62/100\n",
      "Error generating question 63: Invalid control character at: line 10 column 332 (char 1209)\n",
      "Successfully generated question 64/100\n",
      "Successfully generated question 65/100\n",
      "Successfully generated question 66/100\n",
      "Successfully generated question 67/100\n",
      "Successfully generated question 68/100\n",
      "Successfully generated question 69/100\n",
      "Successfully generated question 70/100\n",
      "Error generating question 71: Extra data: line 11 column 2 (char 1419)\n",
      "Successfully generated question 72/100\n",
      "Successfully generated question 73/100\n",
      "Successfully generated question 74/100\n",
      "Successfully generated question 75/100\n",
      "Successfully generated question 76/100\n",
      "Successfully generated question 77/100\n",
      "Successfully generated question 78/100\n",
      "Successfully generated question 79/100\n",
      "Successfully generated question 80/100\n",
      "Successfully generated question 81/100\n",
      "Successfully generated question 82/100\n",
      "Successfully generated question 83/100\n",
      "Successfully generated question 84/100\n",
      "Successfully generated question 85/100\n",
      "Successfully generated question 86/100\n",
      "Successfully generated question 87/100\n",
      "Successfully generated question 88/100\n",
      "Successfully generated question 89/100\n",
      "Successfully generated question 90/100\n",
      "Successfully generated question 91/100\n",
      "Successfully generated question 92/100\n",
      "Successfully generated question 93/100\n",
      "Successfully generated question 94/100\n",
      "Successfully generated question 95/100\n",
      "Successfully generated question 96/100\n",
      "Successfully generated question 97/100\n",
      "Successfully generated question 98/100\n",
      "Successfully generated question 99/100\n",
      "Successfully generated question 100/100\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    questions = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
