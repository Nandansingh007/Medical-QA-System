{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI, OpenAIEmbeddings,ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pinecone import Pinecone\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acessing Pinecone database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 382}},\n",
      " 'total_vector_count': 382}\n"
     ]
    }
   ],
   "source": [
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "## Pinecone Initialization and acessing \n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index_name=\"medicalqabot\"\n",
    "\n",
    "print(pc.Index(index_name).describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_services(index_name,openai_api_key):\n",
    "\n",
    "    ## Pinecone Initialization and acessing \n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    index_name=index_name\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "    ## llm initialization\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0.2,\n",
    "        model_name=\"gpt-4\",\n",
    "        openai_api_key=openai_api_key\n",
    "    )\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "\n",
    "    return index, llm, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mcq_prompt():\n",
    "    \"\"\"Return the chat prompt template for MCQ generation\"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "    Based on the following medical context, create a accurate and unique multiple choice question.\n",
    "    The question should test important clinical concepts and decision-making.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Consider these aspects when generating the question:\n",
    "    1. Focus on specific clinical scenarios rather than general knowledge\n",
    "    2. Include realistic laboratory values, imaging findings, or patient symptoms\n",
    "    3. Make the scenario factual enough to test clinical decision-making\n",
    "    4. Include time-sensitive elements or risk factors that affect the decision\n",
    "    5. Consider including relevant comorbidities or complications\n",
    "    \n",
    "    Generate a multiple choice question in valid JSON format with the following structure:\n",
    "    {{\n",
    "        \"question\": \"Create a detailed clinical scenario that includes:\n",
    "                    - Specific patient demographics\n",
    "                    - Precise symptoms and timeline\n",
    "                    - Relevant lab values or imaging results\n",
    "                    - Important comorbidities\n",
    "                    - Any critical time factors\",\n",
    "        \"options\": [\n",
    "            \"A) A specific, detailed intervention or treatment\",\n",
    "            \"B) An alternative approach with different timing or method\",\n",
    "            \"C) A reasonable but suboptimal choice given the specifics\",\n",
    "            \"D) A common misconception or clearly incorrect approach\"\n",
    "        ],\n",
    "        \"correct_answer\": \"Letter of correct answer (A, B, C, or D)\",\n",
    "        \"reasoning\": \"Provide detailed explanation including:\n",
    "                     1. Why the correct answer is optimal for this specific case\n",
    "                     2. Why each incorrect option is inappropriate\n",
    "                     3. Key clinical factors that influenced the decision\n",
    "                     4. Any relevant guidelines or evidence supporting the choice\"\n",
    "    }}\n",
    "    \n",
    "    Requirements for uniqueness:\n",
    "    1. Avoid basic or commonly tested scenarios\n",
    "    2. Include unique combinations of symptoms or findings\n",
    "    3. Make sure each option is distinct and specific\n",
    "    4. Focus on nuanced clinical decision-making\n",
    "    5. Include recent medical guidelines when relevant\n",
    "    \n",
    "    Return only the JSON object with no additional text or formatting.\n",
    "    \"\"\"\n",
    "    return ChatPromptTemplate.from_template(prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_contexts(index, num_contexts=50):\n",
    "    \"\"\"Retrieve diverse contexts from Pinecone\"\"\"\n",
    "    vectors = index.query(\n",
    "        vector=[0.0] * 1536,  # dummy vector to get all records\n",
    "        top_k=num_contexts,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    return [match.metadata.get('text', '') for match in vectors.matches]\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def calculate_relevance(embeddings, context, question):\n",
    "    \"\"\"Calculate semantic similarity between context and question\"\"\"\n",
    "    context_embedding = embeddings.embed_query(context)\n",
    "    question_embedding = embeddings.embed_query(question)\n",
    "    return float(cosine_similarity(context_embedding, question_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(index, chat_llm, embeddings, num_questions=50):\n",
    "    \"\"\"Generate MCQs from medical contexts\"\"\"\n",
    "    contexts = retrieve_contexts(index, num_questions)\n",
    "    questions = []\n",
    "    prompt = get_mcq_prompt()\n",
    "    \n",
    "    for i, context in enumerate(contexts):\n",
    "        try:\n",
    "            # Generate MCQ using ChatLLM\n",
    "            response = chat_llm.invoke(prompt.format_messages(context=context))\n",
    "            # Extract the content from AIMessage\n",
    "            mcq_str = response.content\n",
    "            # Parse the JSON string\n",
    "            mcq = json.loads(mcq_str)\n",
    "            \n",
    "            # Add metadata for retrieval testing\n",
    "            mcq['context_id'] = i\n",
    "            mcq['relevance_score'] = calculate_relevance(embeddings, context, mcq['question'])\n",
    "            \n",
    "            questions.append(mcq)\n",
    "            print(f\"Successfully generated question {i+1}/{num_questions}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating question {i+1}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_questions(questions, filename=\"medical_mcqs.json\"):\n",
    "    \"\"\"Save generated questions to a JSON file\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(questions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the MCQ generation and testing\"\"\"\n",
    "\n",
    "    # Set up credentials\n",
    "    openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    index_name = \"medicalqabot\" \n",
    "    \n",
    "    # Initialize services\n",
    "    index, llm, embeddings = initialize_services(\n",
    "        index_name,\n",
    "        openai_api_key\n",
    "    )\n",
    "    \n",
    "    # Generate questions\n",
    "    questions = generate_questions(index, llm, embeddings, num_questions=50)\n",
    "    \n",
    "    # Save questions\n",
    "    save_questions(questions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating question 1: Invalid control character at: line 10 column 271 (char 1146)\n",
      "Successfully generated question 2/50\n",
      "Successfully generated question 3/50\n",
      "Successfully generated question 4/50\n",
      "Successfully generated question 5/50\n",
      "Successfully generated question 6/50\n",
      "Successfully generated question 7/50\n",
      "Successfully generated question 8/50\n",
      "Successfully generated question 9/50\n",
      "Successfully generated question 10/50\n",
      "Successfully generated question 11/50\n",
      "Successfully generated question 12/50\n",
      "Successfully generated question 13/50\n",
      "Successfully generated question 14/50\n",
      "Successfully generated question 15/50\n",
      "Successfully generated question 16/50\n",
      "Successfully generated question 17/50\n",
      "Successfully generated question 18/50\n",
      "Successfully generated question 19/50\n",
      "Successfully generated question 20/50\n",
      "Successfully generated question 21/50\n",
      "Successfully generated question 22/50\n",
      "Successfully generated question 23/50\n",
      "Successfully generated question 24/50\n",
      "Successfully generated question 25/50\n",
      "Successfully generated question 26/50\n",
      "Successfully generated question 27/50\n",
      "Successfully generated question 28/50\n",
      "Successfully generated question 29/50\n",
      "Successfully generated question 30/50\n",
      "Successfully generated question 31/50\n",
      "Successfully generated question 32/50\n",
      "Successfully generated question 33/50\n",
      "Successfully generated question 34/50\n",
      "Successfully generated question 35/50\n",
      "Successfully generated question 36/50\n",
      "Successfully generated question 37/50\n",
      "Successfully generated question 38/50\n",
      "Successfully generated question 39/50\n",
      "Successfully generated question 40/50\n",
      "Successfully generated question 41/50\n",
      "Successfully generated question 42/50\n",
      "Successfully generated question 43/50\n",
      "Successfully generated question 44/50\n",
      "Successfully generated question 45/50\n",
      "Successfully generated question 46/50\n",
      "Successfully generated question 47/50\n",
      "Successfully generated question 48/50\n",
      "Successfully generated question 49/50\n",
      "Successfully generated question 50/50\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    questions = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
