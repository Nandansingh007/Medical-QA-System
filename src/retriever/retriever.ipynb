{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.retrievers import MultiQueryRetriever\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.vectorstores.base import VectorStoreRetriever\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing and accessing Pinecone and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_rag_system(index_name, openai_api_key,pinecone_api_key):\n",
    "\n",
    "    \"\"\"Initialize the RAG system with Pinecone, OpenAI embeddings, and LLM.\"\"\"\n",
    "    \n",
    "    ## Pinecone Initialization and acessing \n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    index_name=index_name\n",
    "    index = pc.Index(index_name)\n",
    "    \n",
    "    # Initialize OpenAI embeddings\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    \n",
    "    # Initialize LangChain Pinecone vector store\n",
    "    vectorstore = PineconeVectorStore(\n",
    "        index=index,\n",
    "        embedding=embeddings,\n",
    "        text_key=\"text\"\n",
    "    )\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0.1,\n",
    "        model=\"gpt-4\",\n",
    "        openai_api_key=openai_api_key\n",
    "    )\n",
    "    \n",
    "    return vectorstore, llm, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Find Similar context according to the Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_contexts(question, embeddings, vectorstore):\n",
    "    \"\"\"Get similar contexts using embedding similarity search.\"\"\"\n",
    "    similar_docs = vectorstore.similarity_search_with_score(\n",
    "        question,\n",
    "        k=4\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for doc, score in similar_docs:\n",
    "        results.append({\n",
    "            'context': doc.page_content,\n",
    "            'similarity_score': float(score)\n",
    "        })\n",
    "    \n",
    "    results.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt to use MCQ and retrieved context to find the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_extraction_prompt():\n",
    "    \"\"\"Create prompt for answer extraction.\"\"\"\n",
    "    prompt_template = \"\"\"Based on the following medical question and context, determine the correct answer among the options and explain your reasoning.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "{options}\n",
    "\n",
    "Most Relevant Context (similarity score: {similarity_score:.4f}):\n",
    "{context}\n",
    "\n",
    "Please provide your response in the following format:\n",
    "\n",
    "1. Predicted Answer: [Letter of the correct answer (A, B, C, or D)]\n",
    "2. Confidence: [High/Medium/Low]\n",
    "3. Explanation: [Detailed explanation of why this is the correct answer, citing specific information from the context]\n",
    "4. Relevant Information: [Key pieces of information from the context that led to this answer]\n",
    "\n",
    "Remember to:\n",
    "1. Base your answer solely on the provided context\n",
    "2. If the context doesn't contain enough information, indicate low confidence\n",
    "3. Provide specific citations from the context in your explanation\n",
    "4. Consider both direct evidence and logical implications\n",
    "\"\"\"\n",
    "    return ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Predict MCQ's answer using question, options and highest relevant retrieved context having highest similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(question, options, context, similarity_score, llm):\n",
    "    \"\"\"Predict answer using the most similar context.\"\"\"\n",
    "    prompt = get_answer_extraction_prompt()\n",
    "    options_str = \"\\n\".join(options)\n",
    "    formatted_prompt = prompt.format(\n",
    "        question=question,\n",
    "        options=options_str,\n",
    "        context=context,\n",
    "        similarity_score=similarity_score\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "    response = llm.invoke(messages).content\n",
    "    \n",
    "    result = {}\n",
    "    sections = response.split(\"\\n\")\n",
    "    for section in sections:\n",
    "        if \"Predicted Answer:\" in section:\n",
    "            result[\"predicted_answer\"] = section.split(\"Predicted Answer:\")[1].strip()\n",
    "        elif \"Confidence:\" in section:\n",
    "            result[\"confidence\"] = section.split(\"Confidence:\")[1].strip()\n",
    "        elif \"Explanation:\" in section:\n",
    "            result[\"explanation\"] = section.split(\"Explanation:\")[1].strip()\n",
    "        elif \"Relevant Information:\" in section:\n",
    "            result[\"relevant_info\"] = section.split(\"Relevant Information:\")[1].strip()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to save the QA answer extracted and retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file with timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{filename}_{timestamp}.json\"\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_test():\n",
    "    \"\"\"Main function to test the modified system.\"\"\"\n",
    "    # Load credentials\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "    index_name = \"medicalqabot\"\n",
    "    \n",
    "    # Initialize system\n",
    "    vectorstore, llm, embeddings = initialize_rag_system(index_name, openai_api_key, pinecone_api_key)\n",
    "    \n",
    "    # Sample MCQ\n",
    "    sample_mcq = {\n",
    "        \"question\": \"A 62-year-old female with a history of chronic low back pain, and a recent MRI showed significant lumbar spine degeneration. In addition to the back pain, she complains of bilateral buttock pain and occasional right lower extremity radicular symptoms. She has failed conservative treatments, including physical therapy and NSAIDs. Given her symptoms and the findings of the MRI, what is the most suitable next step in her management?\",\n",
    "        \"options\": [\n",
    "            \"A) Lumbar spine fusion surgery\",\n",
    "            \"B) Sacroiliac joint injection\",\n",
    "            \"C) Preserving the posterior complex during lumbar fusion\",\n",
    "            \"D) Continue with conservative management\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Step 1: Get similar contexts\n",
    "    print(\"\\nStep 1: Finding similar contexts...\")\n",
    "    similar_contexts = get_similar_contexts(sample_mcq['question'], embeddings, vectorstore)\n",
    "    \n",
    "    # Save contexts to JSON\n",
    "    contexts_data = {\n",
    "        \"question\": sample_mcq[\"question\"],\n",
    "        \"similar_contexts\": similar_contexts\n",
    "    }\n",
    "    contexts_file = save_to_json(contexts_data, \"similar_contexts\")\n",
    "    \n",
    "    # Step 2: Predict answer using the most similar context\n",
    "    print(\"\\nStep 2: Predicting answer using most similar context...\")\n",
    "    most_similar_context = similar_contexts[0]\n",
    "    prediction = predict_answer(\n",
    "        sample_mcq['question'],\n",
    "        sample_mcq['options'],\n",
    "        most_similar_context['context'],\n",
    "        most_similar_context['similarity_score'],\n",
    "        llm\n",
    "    )\n",
    "    \n",
    "    # Save complete results to JSON\n",
    "    results_data = {\n",
    "        \"question\": sample_mcq[\"question\"],\n",
    "        \"options\": sample_mcq[\"options\"],\n",
    "        \"most_similar_context\": most_similar_context,\n",
    "        \"prediction\": prediction\n",
    "    }\n",
    "    results_file = save_to_json(results_data, \"qa_results\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Top context similarity score: {most_similar_context['similarity_score']:.4f}\")\n",
    "    print(f\"Predicted answer: {prediction.get('predicted_answer', 'N/A')}\")\n",
    "    print(f\"Confidence: {prediction.get('confidence', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Finding similar contexts...\n",
      "\n",
      "Results saved to: similar_contexts_20250112_221637.json\n",
      "\n",
      "Step 2: Predicting answer using most similar context...\n",
      "\n",
      "Results saved to: qa_results_20250112_221650.json\n",
      "\n",
      "Summary:\n",
      "Top context similarity score: 0.8512\n",
      "Predicted answer: A) Lumbar spine fusion surgery\n",
      "Confidence: Medium\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
